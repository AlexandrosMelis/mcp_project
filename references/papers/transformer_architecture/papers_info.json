{
  "2009.00804v2": {
    "title": "Architectural Implications of Graph Neural Networks",
    "authors": [
      "Zhihui Zhang",
      "Jingwen Leng",
      "Lingxiao Ma",
      "Youshan Miao",
      "Chao Li",
      "Minyi Guo"
    ],
    "summary": "Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.",
    "pdf_url": "https://arxiv.org/pdf/2009.00804v2",
    "published": "2020-09-02"
  },
  "2207.13219v4": {
    "title": "Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications",
    "authors": [
      "Marcelo Orenes-Vera",
      "Esin Tureci",
      "David Wentzlaff",
      "Margaret Martonosi"
    ],
    "summary": "Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.",
    "pdf_url": "https://arxiv.org/pdf/2207.13219v4",
    "published": "2022-07-26"
  },
  "2201.00978v1": {
    "title": "PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture",
    "authors": [
      "Kai Han",
      "Jianyuan Guo",
      "Yehui Tang",
      "Yunhe Wang"
    ],
    "summary": "Transformer networks have achieved great progress for computer vision tasks. Transformer-in-Transformer (TNT) architecture utilizes inner transformer and outer transformer to extract both local and global representations. In this work, we present new TNT baselines by introducing two advanced designs: 1) pyramid architecture, and 2) convolutional stem. The new \"PyramidTNT\" significantly improves the original TNT by establishing hierarchical representations. PyramidTNT achieves better performances than the previous state-of-the-art vision transformers such as Swin Transformer. We hope this new baseline will be helpful to the further research and application of vision transformer. Code will be available at https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch.",
    "pdf_url": "https://arxiv.org/pdf/2201.00978v1",
    "published": "2022-01-04"
  }
}